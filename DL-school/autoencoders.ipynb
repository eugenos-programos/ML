{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1I8kDikouqpH4hf7JBiSYAeNT2IO52T-T\" width=600 height=480/></p>\n<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n\n<h3 style=\"text-align: center;\"><b>Домашнее задание. Весна 2021</b></h3>\n\n# Autoencoders\n","metadata":{"id":"LQ7i1HkmYY68"}},{"cell_type":"markdown","source":"# Часть 1. Vanilla Autoencoder (10 баллов)","metadata":{"id":"Wru2LNFuL2Iq"}},{"cell_type":"markdown","source":"## 1.1. Подготовка данных (0.5 балла)\n","metadata":{"id":"kr3STtdpYY7G"}},{"cell_type":"code","source":"import numpy as np\nfrom torch.autograd import Variable\nfrom torchvision import datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data_utils\nimport torch\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport skimage\nimport skimage.io\nfrom skimage.transform import resize\nfrom tqdm.notebook import tqdm\n\n%matplotlib inline","metadata":{"id":"xTNi9JLRYY7I","execution":{"iopub.status.busy":"2022-05-22T20:59:01.462230Z","iopub.execute_input":"2022-05-22T20:59:01.462533Z","iopub.status.idle":"2022-05-22T20:59:01.471736Z","shell.execute_reply.started":"2022-05-22T20:59:01.462498Z","shell.execute_reply":"2022-05-22T20:59:01.470661Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def fetch_dataset(attrs_name = \"lfw_attributes.txt\",\n                      images_name = \"lfw-deepfunneled\",\n                      dx=80,dy=80,\n                      dimx=64,dimy=64\n    ):\n\n    #download if not exists\n    if not os.path.exists(images_name):\n        print(\"images not found, donwloading...\")\n        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n        print(\"extracting...\")\n        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n        print(\"done\")\n        assert os.path.exists(images_name)\n\n    if not os.path.exists(attrs_name):\n        print(\"attributes not found, downloading...\")\n        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n        print(\"done\")\n\n    #read attrs\n    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n\n\n    #read photos\n    photo_ids = []\n    for dirpath, dirnames, filenames in os.walk(images_name):\n        for fname in filenames:\n            if fname.endswith(\".jpg\"):\n                fpath = os.path.join(dirpath,fname)\n                photo_id = fname[:-4].replace('_',' ').split()\n                person_id = ' '.join(photo_id[:-1])\n                photo_number = int(photo_id[-1])\n                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n\n    photo_ids = pd.DataFrame(photo_ids)\n    # print(photo_ids)\n    #mass-merge\n    #(photos now have same order as attributes)\n    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n\n    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n\n    # print(df.shape)\n    #image preprocessing\n    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n                                .apply(lambda img: resize(img,[dimx,dimy]))\n\n    all_photos = np.stack(all_photos.values)#.astype('uint8')\n    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n    \n    return all_photos, all_attrs","metadata":{"id":"zvAjov5F2NvE","execution":{"iopub.status.busy":"2022-05-22T20:59:01.628968Z","iopub.execute_input":"2022-05-22T20:59:01.629523Z","iopub.status.idle":"2022-05-22T20:59:01.642964Z","shell.execute_reply.started":"2022-05-22T20:59:01.629489Z","shell.execute_reply":"2022-05-22T20:59:01.642208Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\ndata, attrs = fetch_dataset()","metadata":{"id":"W3KhlblLYY7P","execution":{"iopub.status.busy":"2022-05-22T20:59:01.820527Z","iopub.execute_input":"2022-05-22T20:59:01.821222Z","iopub.status.idle":"2022-05-22T20:59:48.523579Z","shell.execute_reply.started":"2022-05-22T20:59:01.821174Z","shell.execute_reply":"2022-05-22T20:59:48.521543Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"\nРазбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:","metadata":{"id":"8MSzXXGoYY7X"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset\n\ndef train_val_dataset(dataset, val_split=.2):\n    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n    datasets = {}\n    datasets['train'] = Subset(dataset, train_idx)\n    datasets['val'] = Subset(dataset, val_idx)\n    return datasets","metadata":{"id":"6NB-sF6hBmgJ","execution":{"iopub.status.busy":"2022-05-22T20:59:48.526705Z","iopub.execute_input":"2022-05-22T20:59:48.527440Z","iopub.status.idle":"2022-05-22T20:59:48.541719Z","shell.execute_reply.started":"2022-05-22T20:59:48.527362Z","shell.execute_reply":"2022-05-22T20:59:48.540716Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"data_dict = train_val_dataset(data)\ntrain_data = data_dict['train']\nval_data = data_dict['val']","metadata":{"scrolled":true,"id":"dFc8lTm_YY7Y","execution":{"iopub.status.busy":"2022-05-22T20:59:48.543084Z","iopub.execute_input":"2022-05-22T20:59:48.544245Z","iopub.status.idle":"2022-05-22T20:59:48.560764Z","shell.execute_reply.started":"2022-05-22T20:59:48.543595Z","shell.execute_reply":"2022-05-22T20:59:48.560062Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Архитектура модели (1.5 балла)\nВ этом разделе мы напишем и обучем обычный автоэнкодер.\n\n\n\n<img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2\" alt=\"Autoencoder\">\n\n\n^ напомню, что автоэнкодер выглядит вот так","metadata":{"id":"z9CC-DUhYY7i"}},{"cell_type":"code","source":"dim_code = 512 # выберите размер латентного вектора","metadata":{"id":"csrNCYh-YY7j","execution":{"iopub.status.busy":"2022-05-22T20:59:48.562871Z","iopub.execute_input":"2022-05-22T20:59:48.563315Z","iopub.status.idle":"2022-05-22T20:59:48.568166Z","shell.execute_reply.started":"2022-05-22T20:59:48.563276Z","shell.execute_reply":"2022-05-22T20:59:48.566651Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"Реализуем autoencoder. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Экспериментируйте!","metadata":{"id":"Fjr-N8AWee-k"}},{"cell_type":"code","source":"pixel_number = 64 ** 2 * 3","metadata":{"execution":{"iopub.status.busy":"2022-05-22T20:59:48.569756Z","iopub.execute_input":"2022-05-22T20:59:48.570603Z","iopub.status.idle":"2022-05-22T20:59:48.577081Z","shell.execute_reply.started":"2022-05-22T20:59:48.570556Z","shell.execute_reply":"2022-05-22T20:59:48.576048Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=3),\n            nn.ReLU(),\n            nn.Conv2d(32, 16, kernel_size=7, stride=1, padding=3),\n            nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(16, 16, kernel_size=7, stride=1, padding=3),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.ConvTranspose2d(16, 32, kernel_size=7, stride=1, padding=3),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 3, kernel_size=7, stride=1, padding=3),\n            nn.Sigmoid()\n        )\n        self.double()\n\n    def forward(self, x):\n        x_lat = self.encoder(x)\n        x = self.decoder(x_lat)\n        return x, x_lat","metadata":{"execution":{"iopub.status.busy":"2022-05-22T20:59:48.578985Z","iopub.execute_input":"2022-05-22T20:59:48.579431Z","iopub.status.idle":"2022-05-22T20:59:48.593976Z","shell.execute_reply.started":"2022-05-22T20:59:48.579392Z","shell.execute_reply":"2022-05-22T20:59:48.592989Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"loss = F.mse_loss\n\ndevice = torch.device('cuda')\n\nmodel = Autoencoder().to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=2e-3)","metadata":{"id":"73lg3bI2YY7m","execution":{"iopub.status.busy":"2022-05-22T20:59:48.595550Z","iopub.execute_input":"2022-05-22T20:59:48.596068Z","iopub.status.idle":"2022-05-22T20:59:48.613450Z","shell.execute_reply.started":"2022-05-22T20:59:48.596030Z","shell.execute_reply":"2022-05-22T20:59:48.612623Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Обучение (2 балла)","metadata":{"id":"GpntmZCe5L6i"}},{"cell_type":"markdown","source":"Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n\nА, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)","metadata":{"id":"Bdxg_3WJYY7o"}},{"cell_type":"code","source":"train_data.dataset = train_data.dataset.transpose(0, 3, 1, 2)\nval_data.dataset = val_data.dataset.transpose(0, 3, 1, 2)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-22T20:59:48.614743Z","iopub.execute_input":"2022-05-22T20:59:48.615186Z","iopub.status.idle":"2022-05-22T20:59:48.620213Z","shell.execute_reply.started":"2022-05-22T20:59:48.615126Z","shell.execute_reply":"2022-05-22T20:59:48.619334Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 64\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\nval_dataloader = DataLoader(val_data, batch_size=batch_size)","metadata":{"id":"AqRMB5bml0kQ","execution":{"iopub.status.busy":"2022-05-22T20:59:48.621626Z","iopub.execute_input":"2022-05-22T20:59:48.622210Z","iopub.status.idle":"2022-05-22T20:59:48.634634Z","shell.execute_reply.started":"2022-05-22T20:59:48.622170Z","shell.execute_reply":"2022-05-22T20:59:48.633678Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T20:59:48.640580Z","iopub.execute_input":"2022-05-22T20:59:48.641108Z","iopub.status.idle":"2022-05-22T20:59:48.648121Z","shell.execute_reply.started":"2022-05-22T20:59:48.641069Z","shell.execute_reply":"2022-05-22T20:59:48.647071Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"n_epochs = 20\nloss_list = []\nfor epoch in tqdm(range(n_epochs)):\n    model.train()\n    for X_train in train_dataloader:\n\n        X_train = X_train.double().to(device)\n        X_pred, x_lat = model(X_train)\n        X_pred = X_pred.to(device)\n        loss_value = loss(X_pred, X_train)\n        \n        loss_value.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        del X_train, X_pred, loss_value, x_lat\n        torch.cuda.empty_cache()\n        \n    model.eval()\n    loss_sum = 0\n    for X_val in val_dataloader:\n\n        X_val = X_val.to(device)\n        X_pred, x_lat = model(X_val)\n        X_pred = X_pred\n        loss_temp = loss(X_pred, X_val)\n        loss_sum += loss_temp.detach().to('cpu')\n        del X_val, X_pred, loss_temp, x_lat\n        torch.cuda.empty_cache()\n    print(f\"Epoch - {epoch}, average loss - {loss_sum / len(val_dataloader)}\")\n    loss_list.append(loss_sum / len(val_dataloader))\n","metadata":{"scrolled":true,"id":"3H3DOojrYY7o","execution":{"iopub.status.busy":"2022-05-22T20:59:48.650103Z","iopub.execute_input":"2022-05-22T20:59:48.650616Z","iopub.status.idle":"2022-05-22T21:01:59.320705Z","shell.execute_reply.started":"2022-05-22T20:59:48.650577Z","shell.execute_reply":"2022-05-22T21:01:59.319130Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"Давайте посмотрим, как наш тренированный автоэнкокодер кодирует и восстанавливает картинки:","metadata":{"id":"FAztAMA4YY7q"}},{"cell_type":"code","source":"plt.plot(loss_list)\nplt.xlabel('epoch')\nplt.ylabel('mse loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T21:01:59.321806Z","iopub.status.idle":"2022-05-22T21:01:59.322783Z","shell.execute_reply.started":"2022-05-22T21:01:59.322503Z","shell.execute_reply":"2022-05-22T21:01:59.322530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = next(iter(val_dataloader)).to(device)\nimage = images[1]\n\nout, x_lat = model(images)","metadata":{"id":"I1J__yvxYY7r","execution":{"iopub.status.busy":"2022-05-22T21:01:59.324239Z","iopub.status.idle":"2022-05-22T21:01:59.324988Z","shell.execute_reply.started":"2022-05-22T21:01:59.324701Z","shell.execute_reply":"2022-05-22T21:01:59.324741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(image) \nplt.show()","metadata":{"id":"c2iFGUdbnqy1","execution":{"iopub.status.busy":"2022-05-22T21:01:59.326318Z","iopub.status.idle":"2022-05-22T21:01:59.327027Z","shell.execute_reply.started":"2022-05-22T21:01:59.326747Z","shell.execute_reply":"2022-05-22T21:01:59.326773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not bad, right? Yeah!","metadata":{"id":"1OPh9O6UYY7s"}},{"cell_type":"markdown","source":"## 1.4. Sampling (2 балла)","metadata":{"id":"dFi96giuYY7t"}},{"cell_type":"markdown","source":"Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n\nДавайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n\n__Подсказка:__Е сли вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как `np.random.randn(25, <latent_space_dim>)`. А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как латентные векторы реальных фоток. Так что в таком случае придется рандом немного подогнать.","metadata":{"id":"AOtUaPNYYY7t"}},{"cell_type":"code","source":"z = np.array([np.random.normal(0, 1, size=(16, 64, 64)) for i in range(24)])\noutput = model.decoder(torch.DoubleTensor(z).to(device))\noutput = output.permute(0, 2, 3, 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T21:01:59.328423Z","iopub.status.idle":"2022-05-22T21:01:59.329142Z","shell.execute_reply.started":"2022-05-22T21:01:59.328870Z","shell.execute_reply":"2022-05-22T21:01:59.328897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = model.encoder(images[0].unsqueeze(0).to(device))\nprint(torch.mean(a), torch.std(a))\ndel a","metadata":{"execution":{"iopub.status.busy":"2022-05-22T21:01:59.330526Z","iopub.status.idle":"2022-05-22T21:01:59.331270Z","shell.execute_reply.started":"2022-05-22T21:01:59.330964Z","shell.execute_reply":"2022-05-22T21:01:59.330991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18, 18))\nfor i in range(output.shape[0]):\n    plt.subplot(output.shape[0] // 2, 2, i + 1)\n    generated = output[i].cpu().detach().numpy()\n    plt.imshow(generated)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T21:01:59.332630Z","iopub.status.idle":"2022-05-22T21:01:59.333364Z","shell.execute_reply.started":"2022-05-22T21:01:59.333074Z","shell.execute_reply":"2022-05-22T21:01:59.333101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latent_tensors = []\nfor image in train_data:\n    image = torch.Tensor(image).to(device)\n    image = image.double().unsqueeze(0)\n    latent_representation = model.encoding(image)\n    latent_tensors.append(latent_representation.detach().cpu())\n    del image","metadata":{"execution":{"iopub.status.busy":"2022-05-22T21:01:59.334779Z","iopub.status.idle":"2022-05-22T21:01:59.335493Z","shell.execute_reply.started":"2022-05-22T21:01:59.335231Z","shell.execute_reply":"2022-05-22T21:01:59.335258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# сгенерируем 25 рандомных векторов размера latent_space\nz = torch.Tensor(np.random.normal(25, 512, 4, 4)).to(device).double()\noutput = model.decoding(z)\n#<выведите тут полученные картинки>\nimg = output[2].squeeze(0).permute(1, 2, 0).detach().cpu()\nplt.imshow(img)","metadata":{"scrolled":true,"id":"8IZykARRYY7u","execution":{"iopub.status.busy":"2022-05-22T21:01:59.336892Z","iopub.status.idle":"2022-05-22T21:01:59.337603Z","shell.execute_reply.started":"2022-05-22T21:01:59.337333Z","shell.execute_reply":"2022-05-22T21:01:59.337360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time to make fun! (4 балла)\n\nДавайте научимся пририсовывать людям улыбки =)","metadata":{"id":"Ey8dD9s0YY7w"}},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">","metadata":{"id":"i1v-8WwuYY7w"}},{"cell_type":"markdown","source":"План такой:\n\n1. Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.\n\nНайти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n\n2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n\n3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n\n4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!","metadata":{"id":"eGE0M2GDYY7x"}},{"cell_type":"code","source":"<ваш код здесь>","metadata":{"id":"f1oBX9EeYY7x","execution":{"iopub.status.busy":"2022-05-22T21:01:59.338933Z","iopub.status.idle":"2022-05-22T21:01:59.339630Z","shell.execute_reply.started":"2022-05-22T21:01:59.339367Z","shell.execute_reply":"2022-05-22T21:01:59.339394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Вуаля! Вы восхитительны!","metadata":{"id":"bXI6jprOYY7z"}},{"cell_type":"markdown","source":"Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в `all_attrs`:)","metadata":{"id":"E2UAf0bpYY70"}},{"cell_type":"markdown","source":"# Часть 2: Variational Autoencoder (10 баллов) ","metadata":{"id":"QQnEGmknYY71"}},{"cell_type":"markdown","source":"Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9","metadata":{"id":"bWQNRjJq2uTz"}},{"cell_type":"code","source":"from torchvision import transforms\n\nbatch_size = 32\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\ntest_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"qBXXr9njByYC","execution":{"iopub.status.busy":"2022-05-22T21:02:06.903795Z","iopub.execute_input":"2022-05-22T21:02:06.904118Z","iopub.status.idle":"2022-05-22T21:02:12.100638Z","shell.execute_reply.started":"2022-05-22T21:02:06.904083Z","shell.execute_reply":"2022-05-22T21:02:12.099856Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Архитектура модели и обучение (2 балла)\n\nРеализуем VAE. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Рекомендуем пользоваться более сложными моделями, чем та, что была на семинаре:) Экспериментируйте!","metadata":{"id":"2rHphW5l8Wgi"}},{"cell_type":"code","source":"features = 16\n# define a simple linear VAE\nclass VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        self.flatten = nn.Flatten()\n \n        # encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(in_features=28 ** 2, out_features=512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=features*2)\n\n        )\n \n        # decoder \n        self.decoder = nn.Sequential(\n            nn.Linear(in_features=features, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=512),\n            nn.ReLU(),\n            nn.Linear(in_features=512, out_features=28 ** 2)        \n        )\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var) # standard deviation\n        eps = torch.randn_like(std) # `randn_like` as we need the same size\n        sample = mu + (eps * std) # sampling as if coming from the input space\n        return sample\n \n    def forward(self, x):\n        # encoding\n        x = self.flatten(x).float()\n        x = self.encoder(x).view(-1, 2, features)\n        # get `mu` and `log_var`\n        mu = x[:, 0, :] # the first feature values as mean\n        log_var = x[:, 1, :] # the other feature values as variance\n        # get the latent vector through reparameterization\n        z = self.reparameterize(mu, log_var)\n \n        # decoding\n        x = self.decoder(z)\n        reconstruction = torch.sigmoid(x)\n        return reconstruction, mu, log_var\n\n    def sample(self, z):\n        generated = self.decoder(z)\n        generated = torch.sigmoid(generated)\n        generated = generated.view(-1, 28, 28, 1)\n        return generated\n\n    def get_latent_vector(self, x):\n        x = self.flatten(x).float()\n        x = self.encoder(x).view(-1, 2, features)\n        # get `mu` and `log_var`\n        mu = x[:, 0, :] # the first feature values as mean\n        log_var = x[:, 1, :] # the other feature values as variance\n        # get the latent vector through reparameterization\n        z = self.reparameterize(mu, log_var)\n        return z","metadata":{"id":"IoNVT5tYYY74","execution":{"iopub.status.busy":"2022-05-22T21:03:57.302664Z","iopub.execute_input":"2022-05-22T21:03:57.302963Z","iopub.status.idle":"2022-05-22T21:03:57.319273Z","shell.execute_reply.started":"2022-05-22T21:03:57.302931Z","shell.execute_reply":"2022-05-22T21:03:57.318298Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"Определим лосс и его компоненты для VAE:","metadata":{"id":"hAB77d-PYY76"}},{"cell_type":"markdown","source":"Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n\nОбщий лосс будет выглядеть так:\n\n$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n\nФормула для KL-дивергенции:\n\n$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n\nВ качестве log-likelihood возьмем привычную нам кросс-энтропию.","metadata":{"id":"UxJrkXGQo5bp"}},{"cell_type":"code","source":"def KL_divergence(mu, logsigma):\n    \"\"\"\n    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n    \"\"\"\n    loss = -0.5 * torch.sum(1 + logsigma - mu.pow(2) - logsigma.exp())\n    return loss\n\ndef log_likelihood(x, reconstruction):\n    \"\"\"\n    часть функции потерь, которая отвечает за качество реконструкции\n    \"\"\"\n    loss = nn.BCELoss(reduction='sum')\n    return loss(reconstruction, x)\n\ndef loss_vae(x, mu, logsigma, reconstruction):\n    return KL_divergence(mu, logsigma) + log_likelihood(x, reconstruction)","metadata":{"id":"ac5ey7uIYY77","execution":{"iopub.status.busy":"2022-05-22T21:02:12.170925Z","iopub.execute_input":"2022-05-22T21:02:12.171281Z","iopub.status.idle":"2022-05-22T21:02:12.181851Z","shell.execute_reply.started":"2022-05-22T21:02:12.171243Z","shell.execute_reply":"2022-05-22T21:02:12.180898Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"И обучим модель:","metadata":{"id":"ZPJQu70eYY79"}},{"cell_type":"code","source":"criterion = loss_vae\n\nautoencoder = VAE().to(device)\n\noptimizer = optim.Adam(autoencoder.parameters())","metadata":{"id":"dtCjfqXdYY79","execution":{"iopub.status.busy":"2022-05-22T21:04:05.148026Z","iopub.execute_input":"2022-05-22T21:04:05.148794Z","iopub.status.idle":"2022-05-22T21:04:05.168683Z","shell.execute_reply.started":"2022-05-22T21:04:05.148753Z","shell.execute_reply":"2022-05-22T21:04:05.167899Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"n_epochs = 10\ntrain_losses = []\nval_losses = []\n\nfor epoch in tqdm(range(n_epochs)):\n    autoencoder.train()\n    train_losses_per_epoch = []\n    for batch, _ in train_loader:\n        optimizer.zero_grad()\n        reconstruction, mu, logsigma = autoencoder(batch.to(device))\n        \n        reconstruction = reconstruction.view(-1, 1, 28, 28)\n        loss = criterion(batch.to(device).float(), mu, logsigma, reconstruction)\n        loss.backward()\n        optimizer.step()\n        train_losses_per_epoch.append(loss.item())\n\n    train_losses.append(np.mean(train_losses_per_epoch))\n\n    autoencoder.eval()\n    val_losses_per_epoch = []\n    with torch.no_grad():\n        for batch, _ in test_loader:\n            reconstruction, mu, logsigma = autoencoder(batch.to(device))\n            reconstruction = reconstruction.view(-1, 1, 28, 28)\n            loss = criterion(batch.to(device).float(), mu, logsigma, reconstruction)\n            val_losses_per_epoch.append(loss.item())\n\n    val_losses.append(np.mean(val_losses_per_epoch))","metadata":{"scrolled":true,"id":"rY1khca6YY7_","execution":{"iopub.status.busy":"2022-05-22T21:04:08.100777Z","iopub.execute_input":"2022-05-22T21:04:08.101049Z","iopub.status.idle":"2022-05-22T21:06:17.509811Z","shell.execute_reply.started":"2022-05-22T21:04:08.101019Z","shell.execute_reply":"2022-05-22T21:06:17.509001Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:","metadata":{"id":"SkxW_8fkYY8B"}},{"cell_type":"code","source":"plt.plot(val_losses)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T21:13:39.984070Z","iopub.execute_input":"2022-05-22T21:13:39.984363Z","iopub.status.idle":"2022-05-22T21:13:40.159744Z","shell.execute_reply.started":"2022-05-22T21:13:39.984332Z","shell.execute_reply":"2022-05-22T21:13:40.158990Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"autoencoder.eval()\nwith torch.no_grad():\n    for batch, _ in test_loader:\n        reconstruction, mu, logsigma = autoencoder(batch.to(device))\n        reconstruction = reconstruction.view(-1, 1, 28, 28)\n        result = reconstruction.cpu().detach().numpy()\n        ground_truth = batch.numpy()\n        break","metadata":{"id":"4Jd3BWM_YY8C","execution":{"iopub.status.busy":"2022-05-22T21:13:51.272569Z","iopub.execute_input":"2022-05-22T21:13:51.272853Z","iopub.status.idle":"2022-05-22T21:13:51.288164Z","shell.execute_reply.started":"2022-05-22T21:13:51.272821Z","shell.execute_reply":"2022-05-22T21:13:51.287373Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 20))\nfor i, (gt, res) in enumerate(zip(ground_truth[:5], result[:5])):\n    plt.subplot(5, 2, 2*i+1)\n    gt = gt.transpose(1, 2, 0)\n    plt.imshow(gt)\n    plt.subplot(5, 2, 2*i+2)\n    res = res.transpose(1, 2, 0)\n    plt.imshow(res)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T21:14:00.666688Z","iopub.execute_input":"2022-05-22T21:14:00.666983Z","iopub.status.idle":"2022-05-22T21:14:01.962803Z","shell.execute_reply.started":"2022-05-22T21:14:00.666950Z","shell.execute_reply":"2022-05-22T21:14:01.962021Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:","metadata":{"id":"PQXYIXjoYY8F"}},{"cell_type":"code","source":"z = np.array([np.random.normal(0, 512, 16) for i in range(10)])\noutput = autoencoder.sample(torch.FloatTensor(z).to(device))\nplt.figure(figsize=(18, 18))\nfor i in range(output.shape[0]):\n  plt.subplot(output.shape[0] // 2, 2, i + 1)\n  generated = output[i].cpu().detach().numpy()\n  plt.imshow(generated)\n\nplt.show()","metadata":{"id":"bOhhH-osYY8G","execution":{"iopub.status.busy":"2022-05-22T21:14:15.864618Z","iopub.execute_input":"2022-05-22T21:14:15.864904Z","iopub.status.idle":"2022-05-22T21:14:17.085605Z","shell.execute_reply.started":"2022-05-22T21:14:15.864871Z","shell.execute_reply":"2022-05-22T21:14:17.084851Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. Latent Representation (2 балла)","metadata":{"id":"Nzt-ENxCr6ul"}},{"cell_type":"markdown","source":"Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.\nВаша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве. \n\nЭто позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве. \n\nПлюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет\n\nПодсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n\n\nИтак, план:\n1. Получить латентные представления картинок тестового датасета\n2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр.","metadata":{"id":"uIWy670xr-Uv"}},{"cell_type":"code","source":"<ваш код получения латентных представлений, применения TSNE и визуализации>","metadata":{"id":"Bk94C6mCsx9c","execution":{"iopub.status.busy":"2022-05-22T21:02:13.477477Z","iopub.status.idle":"2022-05-22T21:02:13.480661Z","shell.execute_reply.started":"2022-05-22T21:02:13.480376Z","shell.execute_reply":"2022-05-22T21:02:13.480407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Что вы думаете о виде латентного представления?","metadata":{"id":"ifxhsvPss5h_"}},{"cell_type":"markdown","source":"__Congrats v2.0!__","metadata":{"id":"ESPBHrL3YY8H"}},{"cell_type":"markdown","source":"## 2.3. Conditional VAE (6 баллов)\n","metadata":{"id":"yIYuKFwijN2U"}},{"cell_type":"markdown","source":"Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер. \nДавайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица). \nИ вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(\n\nХотелось бы добавить к нашему AE функцию \"выдай мне рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n\nИ в этой части задания мы научимся такие обучать.","metadata":{"id":"c5l8Bu1RPjUx"}},{"cell_type":"markdown","source":"### Архитектура\n\nНа картинке ниже представлена архитектура простого Conditional VAE.\n\nПо сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки. \n\nТо есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе.","metadata":{"id":"0j8zNIwKPY-6"}},{"cell_type":"markdown","source":"\n![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)\n\n![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)\n\n","metadata":{"id":"Y6YloFEAPeM4"}},{"cell_type":"markdown","source":"На всякий случай: это VAE, то есть, latent у него все еще состоит из mu и sigma","metadata":{"id":"hxg2tDSfRbLF"}},{"cell_type":"markdown","source":"Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки.","metadata":{"id":"GpFbSXLaPrm1"}},{"cell_type":"markdown","source":"P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе.","metadata":{"id":"cX0zxklMPwI2"}},{"cell_type":"code","source":"class CVAE(nn.Module):\n    def __init__(self):\n        <определите архитектуры encoder и decoder\n        помните, у encoder должны быть два \"хвоста\", \n        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n\n    def encode(self, x, class_num):\n        <реализуйте forward проход энкодера\n        в качестве ваозвращаемых переменных -- mu, logsigma и класс картинки>\n        \n        return mu, logsigma, class_num\n    \n    def gaussian_sampler(self, mu, logsigma):\n        if self.training:\n            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n        else:\n            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n            # на инференсе выход автоэнкодера должен быть детерминирован.\n            return mu\n    \n    def decode(self, z, class_num):\n        <реализуйте forward проход декодера\n        в качестве возвращаемой переменной -- reconstruction>\n        \n        return reconstruction\n\n    def forward(self, x):\n        <используя encode и decode, реализуйте forward проход автоэнкодера\n        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n        return mu, logsigma, reconstruction","metadata":{"id":"ar701cHOkDKS","execution":{"iopub.status.busy":"2022-05-22T21:02:13.481860Z","iopub.status.idle":"2022-05-22T21:02:13.482846Z","shell.execute_reply.started":"2022-05-22T21:02:13.482565Z","shell.execute_reply":"2022-05-22T21:02:13.482598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sampling\n","metadata":{"id":"VoMw-IFyP5A2"}},{"cell_type":"markdown","source":"Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\nДля MNIST вы можете попросить CVAE восстановить из одного латентного вектора, например, картинки цифры 5 и 7.","metadata":{"id":"qe1zWyZHkLV2"}},{"cell_type":"code","source":"<тут нужно научиться сэмплировать из декодера цифры определенного класса>","metadata":{"id":"A0SQIhvNP9Dr","execution":{"iopub.status.busy":"2022-05-22T21:02:13.487100Z","iopub.status.idle":"2022-05-22T21:02:13.488003Z","shell.execute_reply.started":"2022-05-22T21:02:13.487719Z","shell.execute_reply":"2022-05-22T21:02:13.487758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splendid! Вы великолепны!\n","metadata":{"id":"nAWBu8rzQBgQ"}},{"cell_type":"code","source":"<ваш код получения латентных представлений, применения TSNE и визуализации>","metadata":{"execution":{"iopub.status.busy":"2022-05-22T21:02:13.489350Z","iopub.status.idle":"2022-05-22T21:02:13.492597Z","shell.execute_reply.started":"2022-05-22T21:02:13.492326Z","shell.execute_reply":"2022-05-22T21:02:13.492354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Что вы думаете насчет этой картинки? Отличается от картинки для VAE?\n","metadata":{}},{"cell_type":"markdown","source":"### Latent Representations","metadata":{"id":"Rt2S77cm3O1v"}},{"cell_type":"markdown","source":"Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)\n\nОпять же, нужно покрасить точки в разные цвета в зависимости от класса.","metadata":{"id":"Nt7x8Ek_rHTE"}}]}